{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 5\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::424875905672:role/glue-cdl-full-access\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: f9fd1c36-08b8-4157-a680-dc24157face5\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\nWaiting for session f9fd1c36-08b8-4157-a680-dc24157face5 to get into ready status...\nSession f9fd1c36-08b8-4157-a680-dc24157face5 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Aux functions",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.types import *\nfrom pyspark import SQLContext\n\nsqlContext = SQLContext(sc)\n# Auxiliar functions\n# pd to spark (fast)\ndef equivalent_type(f):\n    if f == 'datetime64[ns]': return TimestampType()\n    elif f == 'int64': return LongType()\n    elif f == 'int32' or f == 'uint8': return IntegerType()\n    elif f == 'float64': return DoubleType()\n    elif f == 'float32': return FloatType()\n    else: return StringType()\n\ndef define_structure(string, format_type):\n    try: typo = equivalent_type(format_type)\n    except: typo = StringType()\n    return StructField(string, typo)\n\n# Given pandas dataframe, it will return a spark's dataframe.\ndef pandas_to_spark(pandas_df):\n    columns = list(pandas_df.columns)\n    types = list(pandas_df.dtypes)\n    struct_list = []\n    for column, typo in zip(columns, types): \n      struct_list.append(define_structure(column, typo))\n    p_schema = StructType(struct_list)\n    return sqlContext.createDataFrame(pandas_df, p_schema)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Read",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Read in data as dynamic frame\nharddrive = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"s3\",\n    connection_options={\n        \"paths\": [\"s3://refit-iot/data/harddrive/\"],\n        \"recurse\": True,\n        \"header\": \"true\"\n    },\n    format=\"csv\"\n)\n\n# Convert to spark df\n# All strings\nharddrive_df = harddrive.toDF()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Transform",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "header = harddrive_df.rdd.first()\nharddrive_final = spark.createDataFrame(harddrive_df.rdd.filter(lambda x: x != header), header)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import col, to_date, to_timestamp\n\n# Time: str to timestamp\nharddrive_final = harddrive_final.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n\n# The rest: str to double\ncols_to_cast = harddrive_final.columns[3:]\n\nfor col_name in cols_to_cast:\n    harddrive_final = harddrive_final.withColumn(col_name, col(col_name).cast(\"double\"))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# feature engineering\nfrom pyspark.sql.functions import max\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import datediff\nimport pyspark.sql.functions as F\n\ndf = harddrive_final\nharddrive_failed = df.filter(df.failure == 1).select(\"serial_number\")\n\ndf_analysis = df.join(harddrive_failed.distinct(), on='serial_number', how='inner')\n\nwindowSpec = Window.partitionBy(\"serial_number\")\ndf_analysis = df_analysis.withColumn(\"end_date\", max(col(\"date\")).over(windowSpec))\n\ndf_analysis = df_analysis.withColumn(\"end_date\", F.to_date(col(\"end_date\")))\ndf_analysis = df_analysis.withColumn(\"date\", F.to_date(col(\"date\")))\n\ndf_analysis = df_analysis.withColumn(\"useful_life\", datediff(col(\"end_date\"), col(\"date\")))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Drop NA\ndef drop_null_columns(df):\n    \"\"\"\n    This function drops all columns which contain null values.\n    :param df: A PySpark DataFrame\n    \"\"\"\n    import pyspark.sql.functions as F\n    null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n    to_drop = [k for k, v in null_counts.items() if v > 0]\n    df = df.drop(*to_drop)\n    return df\n\nharddrive_clean = drop_null_columns(df_analysis)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Further drop\nharddrive_out = harddrive_clean.drop('date','serial_number', 'model', 'end_date', 'failure', 'smart_5_normalized', 'smart_198_raw',\n              'smart_198_normalized','smart_199_normalized','smart_241_raw','smart_240_raw','smart_10_raw',\n               'smart_197_normalized','smart_188_raw','smart_12_normalized','smart_10_normalized','smart_7_raw','smart_4_normalized',\n               'smart_242_raw')\nharddrive_out.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- capacity_bytes: double (nullable = true)\n |-- smart_1_normalized: double (nullable = true)\n |-- smart_1_raw: double (nullable = true)\n |-- smart_3_normalized: double (nullable = true)\n |-- smart_3_raw: double (nullable = true)\n |-- smart_4_raw: double (nullable = true)\n |-- smart_5_raw: double (nullable = true)\n |-- smart_7_normalized: double (nullable = true)\n |-- smart_9_normalized: double (nullable = true)\n |-- smart_9_raw: double (nullable = true)\n |-- smart_12_raw: double (nullable = true)\n |-- smart_194_normalized: double (nullable = true)\n |-- smart_194_raw: double (nullable = true)\n |-- smart_197_raw: double (nullable = true)\n |-- smart_199_raw: double (nullable = true)\n |-- useful_life: integer (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "harddrive_out.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "5490\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Write",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import boto3\nfrom awsglue.dynamicframe import DynamicFrame\n\n# Convert to glue dyf\nharddrive_dyf = DynamicFrame.fromDF(harddrive_out, glueContext, 'convert')\n\n# Housekeeping\ndatabase_name = \"harddrive\"\ntable_name = \"streamed\"\nglue_client = boto3.client('glue')\n\n# Define schema\nschema = harddrive_dyf.schema()\ncolumns = [\n    {\n        \"Name\": field.name,\n        \"Type\": field.dataType.typeName()\n    }\n    for field in schema.fields\n]\n\n# Create table configurations\ncreate_table_options_streamed = {\n    \"DatabaseName\": database_name,\n    \"TableInput\": {\n        \"Name\": table_name,\n        \"Description\": \"Streamed data for hard drive failures\",\n        \n        \"StorageDescriptor\": {\n            \"Columns\": columns,\n            \"Location\": \"s3://refit-iot/final_data_landing/harddrive/streamed/\",\n            \"InputFormat\": \"org.apache.hadoop.mapred.TextInputFormat\",\n            \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\n            \"Compressed\": False,\n            \"SerdeInfo\": {\n                \"SerializationLibrary\": \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\n                \"Parameters\": {\n                    \"field.delim\": \",\",\n                    \"skip.header.line.count\" : \"1\"\n                }\n            }\n        },\n        \"PartitionKeys\": []\n    }\n}\n\n# Check if streamed table exists\n# If the streamed table does not exist, create\n\ntry: \n    response = glue_client.get_table(\n    DatabaseName=database_name,\n    Name=table_name\n)\n    print(f\"{table_name} already exists. Directly writing...\")\nexcept:\n    glue_client = boto3.client('glue')\n    response_streamed = glue_client.create_table(**create_table_options_streamed)\n    print(f\"{table_name} does not exist. Creating...\")\n\nglueContext.write_dynamic_frame.from_catalog(\n    frame = harddrive_dyf,\n    database = database_name,\n    table_name = table_name\n    \n)\n\nprint(f\"Sucessfully wrote to {table_name}\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "streamed does not exist. Creating...\nSucessfully wrote to streamed\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}