{
	"metadata": {
		"vscode": {
			"interpreter": {
				"hash": "12eef76e2d2a105dd2d8b4930fec94ff07b63b72c8f772f56fcf111babd83a82"
			}
		},
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 5\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::001898544471:role/glue\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 2b613577-06fb-4617-83fd-6a4ffb401f96\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\nWaiting for session 2b613577-06fb-4617-83fd-6a4ffb401f96 to get into ready status...\nSession 2b613577-06fb-4617-83fd-6a4ffb401f96 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Trips",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Read CSV into PySpark DataFrame\") \\\n    .getOrCreate()\nfile_path = \"s3://usecases-glue-jobs/divvy/static/final_static.csv\"  # Replace this with the path to your CSV file\ntrips_df_static = spark.read.csv(file_path, header=True, inferSchema=True)\ntrips_final_static = trips_df_static.drop(\"_c0\")\n\n# Show the first few rows of the DataFrame\ntrips_df_static.show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------------------+-----+-----+\n|_c0|         start_time|trips|  zip|\n+---+-------------------+-----+-----+\n|  0|2013-06-27 01:00:00|    1|60661|\n|  1|2013-06-27 11:00:00|    1|60622|\n|  2|2013-06-27 11:00:00|    3|60607|\n|  3|2013-06-27 12:00:00|    1|60614|\n|  4|2013-06-27 12:00:00|    2|60611|\n+---+-------------------+-----+-----+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Landmark",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Static and Streamed all in 1 DF\nfrom pyspark.sql.functions import col\n\nfile_path = \"s3://usecases-glue-jobs/divvy/static/landmark_clean.csv\"  # Replace this with the path to your CSV file\nlandmark_df = spark.read.csv(file_path, header=True, inferSchema=True)\nlandmark_df = landmark_df.drop(\"_c0\")\nlandmark_df = landmark_df.withColumn(\"zip_code\", col('zip_code').cast(\"string\"))\n\n# Show the first few rows of the DataFrame\nlandmark_df.show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+---------+\n|zip_code|landmarks|\n+--------+---------+\n|   60302|        1|\n|   60409|        1|\n|   60601|       15|\n|   60602|        9|\n|   60603|       12|\n+--------+---------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Weather",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Static and Streamed all in 1 DF\nfile_path = \"s3://usecases-glue-jobs/divvy/static/weather_static.csv\"  # Replace this with the path to your CSV file\nweather_df_static = spark.read.csv(file_path, header=True, inferSchema=True)\nweather_df_static = weather_df_static.drop(\"_c0\")\n\n# Show the first few rows of the DataFrame\nweather_df_static.show(5)\nweather_df_static.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\n|               time|temp|rel_humidity|dewpoint|apparent_temp|precip|rain|snow|cloudcover|windspeed|\n+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\n|2013-01-01 00:00:00|-4.2|          66|    -9.7|        -10.2|   0.0| 0.0| 0.0|        79|     15.8|\n|2013-01-01 01:00:00|-4.3|          67|    -9.5|        -10.5|   0.0| 0.0| 0.0|        72|     16.1|\n|2013-01-01 02:00:00|-4.4|          67|    -9.7|        -10.3|   0.0| 0.0| 0.0|        82|     14.6|\n|2013-01-01 03:00:00|-4.6|          67|    -9.8|        -10.5|   0.0| 0.0| 0.0|        80|     14.4|\n|2013-01-01 04:00:00|-4.8|          68|    -9.9|        -11.9|   0.0| 0.0| 0.0|        37|     16.3|\n+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\nonly showing top 5 rows\n\nroot\n |-- time: string (nullable = true)\n |-- temp: double (nullable = true)\n |-- rel_humidity: integer (nullable = true)\n |-- dewpoint: double (nullable = true)\n |-- apparent_temp: double (nullable = true)\n |-- precip: double (nullable = true)\n |-- rain: double (nullable = true)\n |-- snow: double (nullable = true)\n |-- cloudcover: integer (nullable = true)\n |-- windspeed: double (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Join",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Weather and trips\nfrom pyspark.sql.functions import col, to_timestamp, date_format\n\nwt_static = trips_final_static.join(weather_df_static, trips_final_static.start_time == weather_df_static.time, \"left\")\n\n# Weather, trips, and landmark\nwtl_static = wt_static.join(landmark_df, wt_static.zip == landmark_df.zip_code, \"left\")\n\n# Drop duplicate\nwtl_static_final = wtl_static.drop(\"zip_code\", \"time\").orderBy(\"start_time\")\nwtl_static_final = wtl_static_final.withColumn(\"start_time\", date_format(\"start_time\", \"yyyy-MM-dd HH:mm:ss\"))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import pyspark.sql.functions as F\n\nspark = SparkSession.builder \\\n    .appName(\"Read CSV into PySpark DataFrame\") \\\n    .getOrCreate()\n\ndf2 = (wtl_static_final.select('zip', F.explode(F.split('zip', ', ')).alias('zip_2'))\n         .groupBy('zip')\n         .pivot('zip')\n         .agg(F.lit(1))\n         .fillna(0)\n      )",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "wtl_static_final = wtl_static_final.withColumnRenamed(\"zip\", \"zip_code\")\nwt_ohe = wtl_static_final.join(df2, wtl_static_final.zip_code == df2.zip, \"left\").orderBy(\"start_time\")\nwt_ohe = wt_ohe.drop(\"zip_code\", \"zip\")\nwt_ohe.show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+-----+----+------------+--------+-------------+------+----+----+----------+---------+---------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n|         start_time|trips|temp|rel_humidity|dewpoint|apparent_temp|precip|rain|snow|cloudcover|windspeed|landmarks|60201|60202|60208|60301|60302|60304|60601|60602|60603|60604|60605|60606|60607|60608|60609|60610|60611|60612|60613|60614|60615|60616|60617|60618|60619|60620|60621|60622|60623|60624|60625|60626|60628|60629|60630|60632|60636|60637|60638|60640|60641|60642|60643|60644|60645|60646|60647|60649|60651|60653|60654|60657|60659|60660|60661|60696|60804|\n+-------------------+-----+----+------------+--------+-------------+------+----+----+----------+---------+---------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n|2013-06-27 01:00:00|    1|22.5|          87|    20.2|         24.6|   0.0| 0.0| 0.0|        34|      6.8|        2|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    1|    0|    0|\n|2013-06-27 11:00:00|    3|26.5|          72|    21.0|         31.9|   0.1| 0.1| 0.0|        36|     10.5|       10|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n|2013-06-27 11:00:00|    1|26.5|          72|    21.0|         31.9|   0.1| 0.1| 0.0|        36|     10.5|        8|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n|2013-06-27 12:00:00|    1|27.2|          70|    21.2|         33.2|   0.0| 0.0| 0.0|        31|     12.9|       14|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n|2013-06-27 12:00:00|    2|27.2|          70|    21.2|         33.2|   0.0| 0.0| 0.0|        31|     12.9|       20|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n+-------------------+-----+----+------------+--------+-------------+------+----+----+----------+---------+---------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import functions as F\nfrom pyspark.sql import types as T\nfrom math import pi\n\n# Make sure start_time is of correct data type\nwt_ohe = wt_ohe.withColumn(\"start_time\", wt_ohe[\"start_time\"].cast(T.TimestampType()))\n\nstart = wt_ohe.select('start_time').first()[0]\n\n# Calculate hours_since_start\ndf_new = wt_ohe.withColumn('hours_since_start', \n                       (F.unix_timestamp('start_time') - F.unix_timestamp(F.lit(start)))/3600)\n\n# Drop start_time column\ndf_new = df_new.drop('start_time')\n\n# Create Year sin, Year cos, Week sin, Week cos, Day sin, Day cos columns\ndf_new = df_new.withColumn('Year_sin', F.sin(df_new['hours_since_start'] * (2 * pi / (365*24))))\ndf_new = df_new.withColumn('Year_cos', F.cos(df_new['hours_since_start'] * (2 * pi / (365*24))))\n\ndf_new = df_new.withColumn('Week_sin', F.sin(df_new['hours_since_start'] * (2 * pi / (7*24))))\ndf_new = df_new.withColumn('Week_cos', F.cos(df_new['hours_since_start'] * (2 * pi / (7*24))))\n\ndf_new = df_new.withColumn('Day_sin', F.sin(df_new['hours_since_start'] * (2 * pi / 24)))\ndf_new = df_new.withColumn('Day_cos', F.cos(df_new['hours_since_start'] * (2 * pi / 24)))",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Spark to Glue",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from awsglue.dynamicframe import DynamicFrame\n\n#Convert from spark df to dynamic frame\nwtl_static_dyf = DynamicFrame.fromDF(df_new, glueContext, 'convert')",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Check if static table exists\n# If the static table does not exist, create\nimport boto3\n\ndatabase_name = \"divvy\"\ntable_name = \"static\"\nglue_client = boto3.client('glue')\n\nschema = wtl_static_dyf.schema()\ncolumns = [\n    {\n        \"Name\": field.name,\n        \"Type\": field.dataType.typeName()\n    }\n    for field in schema.fields\n]\n\n# Create table configurations\ncreate_table_options_static = {\n    \"DatabaseName\": database_name,\n    \"TableInput\": {\n        \"Name\": table_name,\n        \"Description\": \"Streamed data for divvy bikes\",\n        \"StorageDescriptor\": {\n            \"Columns\": columns,\n            \"Location\": \"s3://usecases-glue-jobs/divvy/static/static_to_store/\",\n            \"InputFormat\": \"org.apache.hadoop.mapred.TextInputFormat\",\n            \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\n            \"Compressed\": False,\n            \"SerdeInfo\": {\n                \"SerializationLibrary\": \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\n                \"Parameters\": {\n                    \"field.delim\": \",\"\n                }\n            }\n        },\n        \"PartitionKeys\": []\n    }\n}\n\n\ntry: \n    response = glue_client.get_table(\n    DatabaseName=database_name,\n    Name=table_name\n)\nexcept:\n    glue_client = boto3.client('glue')\n    response_static = glue_client.create_table(**create_table_options_static)\n    print(f\"{table_name} does not exist. Creating...\")\n\nglueContext.write_dynamic_frame.from_catalog(\n    frame = wtl_streamed_dyf,\n    database = \"divvy\",\n    table_name = \"static\",\n    create_dynamic_frame_options={\n        \"type\": \"csv\",\n        \"schema\": wtl_static_dyf.schema()\n    }\n)\n\nprint(f\"Sucessfully wrote to {table_name}\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		}
	]
}