{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \n",
					"output_type": "stream"
				},
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session. \n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0 and 3.0. \n                                      Default: 2.0.\n----\n\n## Selecting Job Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %glue_ray           String        Sets the session type to Glue Ray.\n----\n\n## Glue Config Magic \n*(common across all job types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n----\n\n                                      \n## Magic for Spark Jobs (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n                                      ETL and Streaming support G.1X and G.2X. \n                                      Default: G.1X.\n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n                                      \n## Magic for Ray Job\n\n----\n    %min_workers        Int           The minimum number of workers that are allocated to a Ray job. \n                                      Default: 1.\n    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n                                      Minimum: 0. Maximum: 100.\n    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n                                      Minimum: 0. Maximum: 100.\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n----\n\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2800\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom pyspark import SparkConf\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql.functions import col, to_date, to_timestamp\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\n  \nconfig=SparkConf().set('spark.rpc.message.maxSize','256')\nsc = SparkContext.getOrCreate(conf = config)\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 4a89e243-8e86-4164-a131-05b62ada7cb5.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2800 minutes.\nidle_timeout has been set to 2800 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 4a89e243-8e86-4164-a131-05b62ada7cb5.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 3.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 4a89e243-8e86-4164-a131-05b62ada7cb5.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 4a89e243-8e86-4164-a131-05b62ada7cb5.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 5\nSetting new number of workers to: 5\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "## Aux function",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.types import *\nfrom pyspark import SQLContext\n\nsqlContext = SQLContext(sc)\n# Auxiliar functions\ndef equivalent_type(f):\n    if f == 'datetime64[ns]': return TimestampType()\n    elif f == 'int64': return LongType()\n    elif f == 'int32' or f == 'uint8': return IntegerType()\n    elif f == 'float64': return DoubleType()\n    elif f == 'float32': return FloatType()\n    else: return StringType()\n\ndef define_structure(string, format_type):\n    try: typo = equivalent_type(format_type)\n    except: typo = StringType()\n    return StructField(string, typo)\n\n# Given pandas dataframe, it will return a spark's dataframe.\ndef pandas_to_spark(pandas_df):\n    columns = list(pandas_df.columns)\n    types = list(pandas_df.dtypes)\n    struct_list = []\n    for column, typo in zip(columns, types): \n      struct_list.append(define_structure(column, typo))\n    p_schema = StructType(struct_list)\n    return sqlContext.createDataFrame(pandas_df, p_schema)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Trips",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Read in trips static as dynamic frame\ntrips_static = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"s3\",\n    connection_options={\n        \"paths\": [\"s3://refit-iot/data/divvy/trips_static/\"],\n        \"recurse\": True,\n        \"header\": \"true\"\n    },\n    format=\"csv\"\n)\n\n# Read in trips static as dynamic frame\ntrips_streamed = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"s3\",\n    connection_options={\n        \"paths\": [\"s3://refit-iot/data/divvy/trips_streamed/\"],\n        \"recurse\": True,\n        \"header\": \"true\"\n    },\n    format=\"csv\"\n)\n# Convert to spark df\ntrips_df_static = trips_static.toDF()\ntrips_df_streamed = trips_streamed.toDF()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Static\nheader = trips_df_static.rdd.first()\ntrips_final_static = spark.createDataFrame(trips_df_static.rdd.filter(lambda x: x != header), header)\ntrips_final_static = trips_final_static.drop(\"\")\n\n#Streamed\nheader = trips_df_streamed.rdd.first()\ntrips_final_streamed = spark.createDataFrame(trips_df_streamed.rdd.filter(lambda x: x != header), header)\ntrips_final_streamed = trips_final_streamed.drop(\"\")\n\n# Display the PySpark DataFrame\ntrips_final_static.show(5)\ntrips_final_streamed.show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+-----+-----+\n|         start_time|trips|  zip|\n+-------------------+-----+-----+\n|2013-06-27 01:00:00|    1|60661|\n|2013-06-27 11:00:00|    1|60622|\n|2013-06-27 11:00:00|    3|60607|\n|2013-06-27 12:00:00|    1|60614|\n|2013-06-27 12:00:00|    2|60611|\n+-------------------+-----+-----+\nonly showing top 5 rows\n\n+-------------------+-----+-----+\n|         start_time|trips|  zip|\n+-------------------+-----+-----+\n|2019-06-14 08:00:00|   61|60661|\n|2019-06-14 08:00:00|    1|60202|\n|2019-06-14 08:00:00|   15|60603|\n|2019-06-14 08:00:00|   75|60657|\n|2019-06-14 08:00:00|    4|60641|\n+-------------------+-----+-----+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Static\n# Time: str to timestamp\ntrips_final_static = trips_final_static.withColumn(\"start_time\", to_timestamp(col(\"start_time\"),\"YYYY-MM-DD HH:MM:SS.fffffffff\"))\n\n# trips: str to int\ntrips_final_static = trips_final_static.withColumn(\"trips\", col(\"trips\").cast(\"int\"))\n\n# Streamed\n# Time: str to timestamp\ntrips_final_streamed = trips_final_streamed.withColumn(\"start_time\", to_timestamp(col(\"start_time\"), \"YYYY-MM-DD HH:MM:SS.fffffffff\"))\n\n# trips: str to int\ntrips_final_streamed = trips_final_streamed.withColumn(\"trips\", col(\"trips\").cast(\"int\"))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "trips_final_static.printSchema()\ntrips_final_streamed.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- start_time: timestamp (nullable = true)\n |-- trips: integer (nullable = true)\n |-- zip: string (nullable = true)\n\nroot\n |-- start_time: timestamp (nullable = true)\n |-- trips: integer (nullable = true)\n |-- zip: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Landmark",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Static and Streamed all in 1 DF\nlandmark = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"s3\",\n    connection_options={\n        \"paths\": [\"s3://refit-iot/data/divvy/landmark_csv/dataload=20230423/\"],\n        \"recurse\": True,\n        \"header\": \"true\"\n    },\n    format=\"csv\"\n)\n\n# Convert to spark df\nlandmark_df = landmark.toDF()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Make first row of data header\nheader = landmark_df.rdd.first()\nlandmark_final = spark.createDataFrame(landmark_df.rdd.filter(lambda x: x != header), header)\nlandmark_final = landmark_final.drop(\"\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "landmark_final.show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+---------+\n|zip_code|landmarks|\n+--------+---------+\n|   60302|        1|\n|   60409|        1|\n|   60601|       15|\n|   60602|        9|\n|   60603|       12|\n+--------+---------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Landmark: str to int\nlandmark_final = landmark_final.withColumn(\"landmarks\", col(\"landmarks\").cast(\"int\"))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "landmark_final.show(5)\nlandmark_final.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+---------+\n|zip_code|landmarks|\n+--------+---------+\n|   60302|        1|\n|   60409|        1|\n|   60601|       15|\n|   60602|        9|\n|   60603|       12|\n+--------+---------+\nonly showing top 5 rows\n\nroot\n |-- zip_code: string (nullable = true)\n |-- landmarks: integer (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Weather",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Read in from S3 as dynamic frame\n# Static\nweather_static = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"s3\",\n    connection_options={\n        \"paths\": [\"s3://refit-iot/data/divvy/weather_static/\"],\n        \"recurse\": True,\n        \"header\": \"true\"\n    },\n    format=\"csv\"\n)\n\nweather_streamed = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"s3\",\n    connection_options={\n        \"paths\": [\"s3://refit-iot/data/divvy/weather_streamed/\"],\n        \"recurse\": True,\n        \"header\": \"true\"\n    },\n    format=\"csv\"\n)\n# Convert to spark df\nweather_df_static = weather_static.toDF()\nweather_df_streamed = weather_streamed.toDF()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Make first row of data header\n# Static\nheader = weather_df_static.rdd.first()\nweather_final_static = spark.createDataFrame(weather_df_static.rdd.filter(lambda x: x != header), header)\nweather_final_static = weather_final_static.drop(\"\")\n\n#Streamed\nheader = weather_df_streamed.rdd.first()\nweather_final_streamed = spark.createDataFrame(weather_df_streamed.rdd.filter(lambda x: x != header), header)\nweather_final_streamed = weather_final_streamed.drop(\"\")\n\n# Display the PySpark DataFrame\nweather_final_static.show(5)\nweather_final_streamed.show(5)",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\n|               time|temp|rel_humidity|dewpoint|apparent_temp|precip|rain|snow|cloudcover|windspeed|\n+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\n|2013-01-01 00:00:00|-4.2|          66|    -9.7|        -10.2|   0.0| 0.0| 0.0|        79|     15.8|\n|2013-01-01 01:00:00|-4.3|          67|    -9.5|        -10.5|   0.0| 0.0| 0.0|        72|     16.1|\n|2013-01-01 02:00:00|-4.4|          67|    -9.7|        -10.3|   0.0| 0.0| 0.0|        82|     14.6|\n|2013-01-01 03:00:00|-4.6|          67|    -9.8|        -10.5|   0.0| 0.0| 0.0|        80|     14.4|\n|2013-01-01 04:00:00|-4.8|          68|    -9.9|        -11.9|   0.0| 0.0| 0.0|        37|     16.3|\n+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\nonly showing top 5 rows\n\n+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\n|               time|temp|rel_humidity|dewpoint|apparent_temp|precip|rain|snow|cloudcover|windspeed|\n+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\n|2019-06-14 08:00:00|16.3|          64|     9.6|         14.8|   0.0| 0.0| 0.0|         7|     18.2|\n|2019-06-14 09:00:00|18.0|          59|     9.9|         17.4|   0.0| 0.0| 0.0|        42|     20.0|\n|2019-06-14 10:00:00|19.2|          54|     9.7|         19.1|   0.0| 0.0| 0.0|        26|     21.6|\n|2019-06-14 11:00:00|20.1|          51|     9.6|         20.2|   0.0| 0.0| 0.0|        27|     23.2|\n|2019-06-14 12:00:00|20.7|          49|     9.8|         21.0|   0.0| 0.0| 0.0|        24|     25.0|\n+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Static\n# Time: str to timestamp\nweather_final_static = weather_final_static.withColumn(\"time\", to_timestamp(col(\"time\"), \"YYYY-MM-DD HH:MM:SS.fffffffff\"))\n\n# The rest: str to double\ncols_to_cast = weather_final_static.columns[1:]\nfor col_name in cols_to_cast:\n    weather_final_static = weather_final_static.withColumn(col_name, col(col_name).cast(\"double\"))\n\n# Streamed\n# Time: str to timestamp\nweather_final_streamed = weather_final_streamed.withColumn(\"time\", to_timestamp(col(\"time\"), \"YYYY-MM-DD HH:MM:SS.fffffffff\"))\n\n# The rest: str to double\ncols_to_cast = weather_final_streamed.columns[1:]\nfor col_name in cols_to_cast:\n    weather_final_streamed = weather_final_streamed.withColumn(col_name, col(col_name).cast(\"double\"))\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "weather_final_static.show(5)\nweather_final_static.printSchema()\n\nweather_final_streamed.show(5)\nweather_final_streamed.printSchema()",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\n|               time|temp|rel_humidity|dewpoint|apparent_temp|precip|rain|snow|cloudcover|windspeed|\n+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\n|2013-01-01 00:00:00|-4.2|        66.0|    -9.7|        -10.2|   0.0| 0.0| 0.0|      79.0|     15.8|\n|2013-01-01 01:00:00|-4.3|        67.0|    -9.5|        -10.5|   0.0| 0.0| 0.0|      72.0|     16.1|\n|2013-01-01 02:00:00|-4.4|        67.0|    -9.7|        -10.3|   0.0| 0.0| 0.0|      82.0|     14.6|\n|2013-01-01 03:00:00|-4.6|        67.0|    -9.8|        -10.5|   0.0| 0.0| 0.0|      80.0|     14.4|\n|2013-01-01 04:00:00|-4.8|        68.0|    -9.9|        -11.9|   0.0| 0.0| 0.0|      37.0|     16.3|\n+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\nonly showing top 5 rows\n\nroot\n |-- time: timestamp (nullable = true)\n |-- temp: double (nullable = true)\n |-- rel_humidity: double (nullable = true)\n |-- dewpoint: double (nullable = true)\n |-- apparent_temp: double (nullable = true)\n |-- precip: double (nullable = true)\n |-- rain: double (nullable = true)\n |-- snow: double (nullable = true)\n |-- cloudcover: double (nullable = true)\n |-- windspeed: double (nullable = true)\n\n+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\n|               time|temp|rel_humidity|dewpoint|apparent_temp|precip|rain|snow|cloudcover|windspeed|\n+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\n|2019-06-14 08:00:00|16.3|        64.0|     9.6|         14.8|   0.0| 0.0| 0.0|       7.0|     18.2|\n|2019-06-14 09:00:00|18.0|        59.0|     9.9|         17.4|   0.0| 0.0| 0.0|      42.0|     20.0|\n|2019-06-14 10:00:00|19.2|        54.0|     9.7|         19.1|   0.0| 0.0| 0.0|      26.0|     21.6|\n|2019-06-14 11:00:00|20.1|        51.0|     9.6|         20.2|   0.0| 0.0| 0.0|      27.0|     23.2|\n|2019-06-14 12:00:00|20.7|        49.0|     9.8|         21.0|   0.0| 0.0| 0.0|      24.0|     25.0|\n+-------------------+----+------------+--------+-------------+------+----+----+----------+---------+\nonly showing top 5 rows\n\nroot\n |-- time: timestamp (nullable = true)\n |-- temp: double (nullable = true)\n |-- rel_humidity: double (nullable = true)\n |-- dewpoint: double (nullable = true)\n |-- apparent_temp: double (nullable = true)\n |-- precip: double (nullable = true)\n |-- rain: double (nullable = true)\n |-- snow: double (nullable = true)\n |-- cloudcover: double (nullable = true)\n |-- windspeed: double (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Join",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Weather and trips\nwt_static = trips_final_static.join(weather_final_static, trips_final_static.start_time == weather_final_static.time, \"left\")\nwt_streamed = trips_final_streamed.join(weather_final_streamed, trips_final_streamed.start_time == weather_final_streamed.time, \"left\")\n\n# Weather, trips, and landmark\nwtl_static = wt_static.join(landmark_final, wt_static.zip == landmark_final.zip_code, \"left\")\nwtl_streamed = wt_streamed.join(landmark_final, wt_streamed.zip == landmark_final.zip_code, \"left\")\n\n# Drop duplicate\nwtl_static_final = wtl_static.drop(\"zip\", \"time\").orderBy(\"start_time\")\nwtl_streamed_final = wtl_streamed.drop(\"zip\", \"time\").orderBy(\"start_time\")\n\n# Check\nwtl_static_final.show(5)\nwtl_static_final.printSchema()\nwtl_streamed_final.show(5)\nwtl_streamed_final.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+-----+----+------------+--------+-------------+------+----+----+----------+---------+--------+---------+\n|         start_time|trips|temp|rel_humidity|dewpoint|apparent_temp|precip|rain|snow|cloudcover|windspeed|zip_code|landmarks|\n+-------------------+-----+----+------------+--------+-------------+------+----+----+----------+---------+--------+---------+\n|2013-06-27 01:00:00|    1|22.5|        87.0|    20.2|         24.6|   0.0| 0.0| 0.0|      34.0|      6.8|   60661|        2|\n|2013-06-27 11:00:00|    1|26.5|        72.0|    21.0|         31.9|   0.1| 0.1| 0.0|      36.0|     10.5|   60622|        8|\n|2013-06-27 11:00:00|    3|26.5|        72.0|    21.0|         31.9|   0.1| 0.1| 0.0|      36.0|     10.5|   60607|       10|\n|2013-06-27 12:00:00|    2|27.2|        70.0|    21.2|         33.2|   0.0| 0.0| 0.0|      31.0|     12.9|   60611|       20|\n|2013-06-27 12:00:00|    1|27.2|        70.0|    21.2|         33.2|   0.0| 0.0| 0.0|      31.0|     12.9|   60614|       14|\n+-------------------+-----+----+------------+--------+-------------+------+----+----+----------+---------+--------+---------+\nonly showing top 5 rows\n\nroot\n |-- start_time: timestamp (nullable = true)\n |-- trips: integer (nullable = true)\n |-- temp: double (nullable = true)\n |-- rel_humidity: double (nullable = true)\n |-- dewpoint: double (nullable = true)\n |-- apparent_temp: double (nullable = true)\n |-- precip: double (nullable = true)\n |-- rain: double (nullable = true)\n |-- snow: double (nullable = true)\n |-- cloudcover: double (nullable = true)\n |-- windspeed: double (nullable = true)\n |-- zip_code: string (nullable = true)\n |-- landmarks: integer (nullable = true)\n\n+-------------------+-----+----+------------+--------+-------------+------+----+----+----------+---------+--------+---------+\n|         start_time|trips|temp|rel_humidity|dewpoint|apparent_temp|precip|rain|snow|cloudcover|windspeed|zip_code|landmarks|\n+-------------------+-----+----+------------+--------+-------------+------+----+----+----------+---------+--------+---------+\n|2019-06-14 08:00:00|    1|16.3|        64.0|     9.6|         14.8|   0.0| 0.0| 0.0|       7.0|     18.2|    null|     null|\n|2019-06-14 08:00:00|   15|16.3|        64.0|     9.6|         14.8|   0.0| 0.0| 0.0|       7.0|     18.2|   60626|        1|\n|2019-06-14 08:00:00|   15|16.3|        64.0|     9.6|         14.8|   0.0| 0.0| 0.0|       7.0|     18.2|   60603|       12|\n|2019-06-14 08:00:00|    1|16.3|        64.0|     9.6|         14.8|   0.0| 0.0| 0.0|       7.0|     18.2|    null|     null|\n|2019-06-14 08:00:00|   61|16.3|        64.0|     9.6|         14.8|   0.0| 0.0| 0.0|       7.0|     18.2|   60661|        2|\n+-------------------+-----+----+------------+--------+-------------+------+----+----+----------+---------+--------+---------+\nonly showing top 5 rows\n\nroot\n |-- start_time: timestamp (nullable = true)\n |-- trips: integer (nullable = true)\n |-- temp: double (nullable = true)\n |-- rel_humidity: double (nullable = true)\n |-- dewpoint: double (nullable = true)\n |-- apparent_temp: double (nullable = true)\n |-- precip: double (nullable = true)\n |-- rain: double (nullable = true)\n |-- snow: double (nullable = true)\n |-- cloudcover: double (nullable = true)\n |-- windspeed: double (nullable = true)\n |-- zip_code: string (nullable = true)\n |-- landmarks: integer (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import pandas as pd\n\n# Turn spark into pd\nwtl_pd_static = wtl_static_final.toPandas()\nwtl_pd_streamed = wtl_streamed_final.toPandas()\n\n# One-hot encode zipcode\nstatic_ohe = pd.get_dummies(wtl_pd_static[\"zip_code\"])\nstreamed_ohe = pd.get_dummies(wtl_pd_streamed[\"zip_code\"])\n\n# Combine\nwtl_pd_static = pd.concat([wtl_pd_static, static_ohe], axis = 1)\nwtl_pd_streamed = pd.concat([wtl_pd_streamed, streamed_ohe], axis = 1)\n\n# Drop original\nwtl_pd_static.drop(labels = 'zip_code', axis = 1, inplace = True)\nwtl_pd_streamed.drop(labels = 'zip_code', axis = 1, inplace = True)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "wtl_pd_streamed.columns",
			"metadata": {
				"trusted": true
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "Index(['start_time', 'trips', 'temp', 'rel_humidity', 'dewpoint',\n       'apparent_temp', 'precip', 'rain', 'snow', 'cloudcover', 'windspeed',\n       'landmarks', '60302', '60601', '60602', '60603', '60604', '60605',\n       '60606', '60607', '60608', '60609', '60610', '60611', '60612', '60613',\n       '60614', '60615', '60616', '60617', '60618', '60619', '60620', '60621',\n       '60622', '60623', '60624', '60625', '60626', '60628', '60629', '60630',\n       '60632', '60636', '60637', '60640', '60641', '60642', '60643', '60644',\n       '60645', '60647', '60649', '60651', '60653', '60654', '60657', '60659',\n       '60660', '60661'],\n      dtype='object')\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Back to spark and fill NaN\nwtl_static_final = pandas_to_spark(wtl_pd_static).fillna(0)\nwtl_streamed_final = pandas_to_spark(wtl_pd_streamed).fillna(0)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Convert spark df to glue dynamic frame",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from awsglue.dynamicframe import DynamicFrame\n\n#Convert from spark df to dynamic frame\nwtl_static_dyf = DynamicFrame.fromDF(wtl_static_final, glueContext, 'convert')\nwtl_streamed_dyf = DynamicFrame.fromDF(wtl_streamed_final, glueContext, 'convert')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Coalesce output into 1 file\n# coalesced_wtl_static = wtl_static_dyf.coalesce()\n# coalesced_wtl_streamed = wtl_streamed_dyf.coalesce(1)\n\n# Write to S3\n# Static\n# glueContext.write_dynamic_frame.from_options(\n#     frame = wtl_static_dyf,\n#     connection_type = 's3',\n#     connection_options = {'path': 's3://refit-iot/final_data_landing/static/'},\n#     format = 'csv',\n#     format_options = {\n#         'separator': ','\n#     },\n#     transformation_ctx = 'datasink2'\n# )\n\n# # Streamed\n# glueContext.write_dynamic_frame.from_options(\n#     frame = wtl_streamed_dyf,\n#     connection_type = 's3',\n#     connection_options = {'path': 's3://refit-iot/final_data_landing/streamed/'},\n#     format = 'csv',\n#     format_options = {\n#         'separator': ','\n#     },\n#     transformation_ctx = 'datasink2'\n# )",
			"metadata": {},
			"execution_count": null,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f3299f14e50>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import boto3\n\n# Housekeeping\ndatabase_name = \"divvy\"\ntable_name = \"streamed\"\nglue_client = boto3.client('glue')\n\n# Define schema\nschema = wtl_streamed_dyf.schema()\ncolumns = [\n    {\n        \"Name\": field.name,\n        \"Type\": field.dataType.typeName()\n    }\n    for field in schema.fields\n]\n\n# Create table configurations\ncreate_table_options_streamed = {\n    \"DatabaseName\": database_name,\n    \"TableInput\": {\n        \"Name\": table_name,\n        \"Description\": \"Streamed data for divvy bikes\",\n        \n        \"StorageDescriptor\": {\n            \"Columns\": columns,\n            \"Location\": \"s3://refit-iot/final_data_landing/streamed/\",\n            \"InputFormat\": \"org.apache.hadoop.mapred.TextInputFormat\",\n            \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\n            \"Compressed\": False,\n            \"SerdeInfo\": {\n                \"SerializationLibrary\": \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\n                \"Parameters\": {\n                    \"dateTimeFormat\": \"YYYY-MM-DD HH:MM:SS.fffffffff\",\n                    \"field.delim\": \",\",\n                    \"skip.header.line.count\" : \"1\"\n                }\n            }\n        },\n        \"PartitionKeys\": []\n    }\n}\n\n# Check if streamed table exists\n# If the streamed table does not exist, create\n\ntry: \n    response = glue_client.get_table(\n    DatabaseName=database_name,\n    Name=table_name\n)\n    print(f\"{table_name} already exists. Directly writing...\")\nexcept:\n    glue_client = boto3.client('glue')\n    response_streamed = glue_client.create_table(**create_table_options_streamed)\n    print(f\"{table_name} does not exist. Creating...\")\n\nglueContext.write_dynamic_frame.from_catalog(\n    frame = wtl_streamed_dyf,\n    database = \"divvy\",\n    table_name = \"streamed\"\n    \n)\n\nprint(f\"Sucessfully wrote to {table_name}\")\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 41,
			"outputs": [
				{
					"name": "stdout",
					"text": "streamed does not exist. Creating...\nSucessfully wrote to streamed\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "wtl_streamed_dyf.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 38,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- start_time: timestamp\n|-- trips: int\n|-- temp: double\n|-- rel_humidity: double\n|-- dewpoint: double\n|-- apparent_temp: double\n|-- precip: double\n|-- rain: double\n|-- snow: double\n|-- cloudcover: double\n|-- windspeed: double\n|-- landmarks: double\n|-- 60302: int\n|-- 60601: int\n|-- 60602: int\n|-- 60603: int\n|-- 60604: int\n|-- 60605: int\n|-- 60606: int\n|-- 60607: int\n|-- 60608: int\n|-- 60609: int\n|-- 60610: int\n|-- 60611: int\n|-- 60612: int\n|-- 60613: int\n|-- 60614: int\n|-- 60615: int\n|-- 60616: int\n|-- 60617: int\n|-- 60618: int\n|-- 60619: int\n|-- 60620: int\n|-- 60621: int\n|-- 60622: int\n|-- 60623: int\n|-- 60624: int\n|-- 60625: int\n|-- 60626: int\n|-- 60628: int\n|-- 60629: int\n|-- 60630: int\n|-- 60632: int\n|-- 60636: int\n|-- 60637: int\n|-- 60640: int\n|-- 60641: int\n|-- 60642: int\n|-- 60643: int\n|-- 60644: int\n|-- 60645: int\n|-- 60647: int\n|-- 60649: int\n|-- 60651: int\n|-- 60653: int\n|-- 60654: int\n|-- 60657: int\n|-- 60659: int\n|-- 60660: int\n|-- 60661: int\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "wtl_streamed_dyf.toDF().toPandas().dtypes",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": [
				{
					"name": "stdout",
					"text": "Execution Interrupted. Attempting to cancel the statement (statement_id=40)\nStatement 40 has been cancelled\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "database_name = \"divvy\"\ntable_name = \"streamed\"\ntest = glueContext.create_dynamic_frame.from_catalog(\n    database = database_name,\n    table_name = table_name,\n    additional_options={\"skip.header.line.count\": \"1\"}\n)\n    \n# test.printSchema()\n\nfiltered_df = Filter.apply(frame = test, f = lambda x: x[\"trips\"] != \"trips\")\nfiltered_df.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 42,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- 60626: int\n|-- 60632: int\n|-- 60653: int\n|-- 60647: int\n|-- rain: double\n|-- 60620: int\n|-- 60608: int\n|-- 60629: int\n|-- 60614: int\n|-- 60641: int\n|-- 60623: int\n|-- 60617: int\n|-- 60602: int\n|-- 60611: int\n|-- 60605: int\n|-- 60661: int\n|-- apparent_temp: double\n|-- 60302: int\n|-- 60637: int\n|-- 60643: int\n|-- 60610: int\n|-- 60619: int\n|-- 60625: int\n|-- 60628: int\n|-- 60649: int\n|-- 60607: int\n|-- 60640: int\n|-- 60613: int\n|-- start_time: string\n|-- cloudcover: double\n|-- 60622: int\n|-- 60601: int\n|-- 60616: int\n|-- temp: double\n|-- rel_humidity: double\n|-- dewpoint: double\n|-- trips: int\n|-- 60604: int\n|-- 60660: int\n|-- 60654: int\n|-- windspeed: double\n|-- 60621: int\n|-- 60642: int\n|-- 60636: int\n|-- 60657: int\n|-- 60624: int\n|-- 60630: int\n|-- 60603: int\n|-- snow: double\n|-- 60645: int\n|-- 60651: int\n|-- 60618: int\n|-- 60612: int\n|-- 60606: int\n|-- precip: double\n|-- 60609: int\n|-- landmarks: double\n|-- 60615: int\n|-- 60659: int\n|-- 60644: int\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Check if static table exists\n# If the static table does not exist, create\ndatabase_name = \"divvy\"\ntable_name = \"static\"\nglue_client = boto3.client('glue')\n\nschema = wtl_static_dyf.schema()\ncolumns = [\n    {\n        \"Name\": field.name,\n        \"Type\": field.dataType.typeName()\n    }\n    for field in schema.fields\n]\n\n# Create table configurations\ncreate_table_options_static = {\n    \"DatabaseName\": database_name,\n    \"TableInput\": {\n        \"Name\": table_name,\n        \"Description\": \"Streamed data for divvy bikes\",\n        \"StorageDescriptor\": {\n            \"Columns\": columns,\n            \"Location\": \"s3://refit-iot/final_data_landing/static/\",\n            \"InputFormat\": \"org.apache.hadoop.mapred.TextInputFormat\",\n            \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\n            \"Compressed\": False,\n            \"SerdeInfo\": {\n                \"SerializationLibrary\": \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\n                \"Parameters\": {\n                    \"field.delim\": \",\"\n                }\n            }\n        },\n        \"PartitionKeys\": []\n    }\n}\n\n\ntry: \n    response = glue_client.get_table(\n    DatabaseName=database_name,\n    Name=table_name\n)\nexcept:\n    glue_client = boto3.client('glue')\n    response_static = glue_client.create_table(**create_table_options_static)\n    print(f\"{table_name} does not exist. Creating...\")\n\nglueContext.write_dynamic_frame.from_catalog(\n    frame = wtl_streamed_dyf,\n    database = \"divvy\",\n    table_name = \"static\",\n    create_dynamic_frame_options={\n        \"type\": \"csv\",\n        \"schema\": wtl_static_dyf.schema()\n    }\n)\n\nprint(f\"Sucessfully wrote to {table_name}\")",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}