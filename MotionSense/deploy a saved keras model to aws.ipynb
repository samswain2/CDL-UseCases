{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy your pre-trained keras model to AWS\n",
    "adapted from https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Upload your model to SageMaker\n",
    "This is a manual step. \n",
    "\n",
    "(If you don't have a model, deel free to use my model below - simply uncomment and run the `wget` statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget https://raw.githubusercontent.com/liampearson/Youtube/master/Keras%20to%20aws%20SageMaker/models/model.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. input the model names\n",
    "complete either:\n",
    "- MODEL_LOCATION.  or\n",
    "- both of JSON and WEIGHTS_LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# if your model is saved as only a .h5 file\n",
    "MODEL_LOCATION ='MotionSense_LSTM.h5'\n",
    "\n",
    "# or if your model is saved as 2 files: model as a .json file, and weights as a .h5 file\n",
    "JSON_LOCATION = ''\n",
    "WEIGHTS_LOCATION = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Your Model\n",
    "Simply run the cell below; the model will be loaded based on how you defined the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model from MODEL_LOCATION\n"
     ]
    }
   ],
   "source": [
    "if MODEL_LOCATION!='': #if your model is saved as a .h5 file only\n",
    "    from keras.models import load_model\n",
    "    model = load_model(MODEL_LOCATION) #load the model\n",
    "    print(\"loaded model from MODEL_LOCATION\")\n",
    "    \n",
    "elif JSON_LOCATION!='': # you have your model saved as a JSON file AND weights\n",
    "#adapted from https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open(JSON_LOCATION, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    \n",
    "    model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights(WEIGHTS_LOCATION)\n",
    "    print(\"loaded model from JSON_LOCATION and WEIGHTS_LOCATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert the Keras Model to the format AWS wants\n",
    "- Converts to a Protobuff file\n",
    "- Saves it in a certain aws file structure\n",
    "- Tarballs this file and zips it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: export/Servo/1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: export/Servo/1\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def convert_h5_to_aws(loaded_model):\n",
    "    \"\"\"\n",
    "    given a pre-trained keras model, this function converts it to a TF protobuf format\n",
    "    and saves it in the file structure which AWS expects\n",
    "    \"\"\"\n",
    "    \n",
    "    # This is the file structure which AWS expects. Cannot be changed.\n",
    "    model_version = '1'\n",
    "    export_dir = 'export/Servo/' + model_version\n",
    "\n",
    "    # Get model input and output signature\n",
    "    model_input = loaded_model.input\n",
    "    model_output = loaded_model.output\n",
    "\n",
    "    # Create a concrete function from the Keras model\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=model_input.shape, dtype=tf.float32)])\n",
    "    def serving_model(inputs):\n",
    "        return {\"score\": loaded_model(inputs)}\n",
    "\n",
    "    # Get the concrete function\n",
    "    concrete_function = serving_model.get_concrete_function()\n",
    "\n",
    "    # Save the model in the TensorFlow SavedModel format\n",
    "    tf.saved_model.save(\n",
    "        loaded_model,\n",
    "        export_dir,\n",
    "        signatures=concrete_function\n",
    "    )\n",
    "\n",
    "    # Create a tarball/tar file and zip it\n",
    "    import tarfile\n",
    "    with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "        archive.add('export', recursive=True)\n",
    "\n",
    "# Replace \"model\" with your actual Keras model\n",
    "convert_h5_to_aws(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Move the tarball (tar.gz) to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_16472\\1973582755.py\", line 1, in <module>\n",
      "    import sagemaker\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\sagemaker\\__init__.py\", line 18, in <module>\n",
      "    from sagemaker import estimator, parameter, tuner  # noqa: F401\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\sagemaker\\estimator.py\", line 30, in <module>\n",
      "    from sagemaker import git_utils, image_uris, vpc_utils\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\sagemaker\\image_uris.py\", line 25, in <module>\n",
      "    from sagemaker.spark import defaults\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\sagemaker\\spark\\__init__.py\", line 16, in <module>\n",
      "    from sagemaker.spark.processing import PySparkProcessor, SparkJarProcessor  # noqa: F401\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\sagemaker\\spark\\processing.py\", line 38, in <module>\n",
      "    from sagemaker.local.image import _ecr_login_if_needed, _pull_image\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\sagemaker\\local\\__init__.py\", line 16, in <module>\n",
      "    from .local_session import (  # noqa: F401\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\sagemaker\\local\\local_session.py\", line 25, in <module>\n",
      "    from sagemaker.local.image import _SageMakerContainer\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\sagemaker\\local\\image.py\", line 38, in <module>\n",
      "    import sagemaker.local.data\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\sagemaker\\local\\data.py\", line 26, in <module>\n",
      "    import sagemaker.amazon.common\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\sagemaker\\amazon\\common.py\", line 23, in <module>\n",
      "    from sagemaker.amazon.record_pb2 import Record\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\sagemaker\\amazon\\record_pb2.py\", line 36, in <module>\n",
      "    _descriptor.FieldDescriptor(\n",
      "  File \"c:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\protobuf\\descriptor.py\", line 561, in __new__\n",
      "TypeError: Descriptors cannot not be created directly.\n",
      "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
      "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
      " 1. Downgrade the protobuf package to 3.20.x or lower.\n",
      " 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
      "\n",
      "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1177, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1030, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 960, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 870, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 704, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\Sam\\AppData\\Roaming\\Python\\Python311\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the name of the bucket which SageMaker made in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket name is:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sagemaker_session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# where did it upload to?\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBucket name is:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m sagemaker_session\u001b[39m.\u001b[39mdefault_bucket()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sagemaker_session' is not defined"
     ]
    }
   ],
   "source": [
    "# where did it upload to?\n",
    "print(\"Bucket name is:\")\n",
    "sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a SageMaker Model\n",
    "First, create an empty train.py file (TensorFlowModel expects this at its 'entry point', but can be empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch train.py #create an empty python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# the (default) IAM role you created when creating this notebook\n",
    "role = get_execution_role()\n",
    "\n",
    "# Create a Sagemaker model (see AWS console>SageMaker>Models)\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "sagemaker_model = TensorFlowModel(model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model.tar.gz',\n",
    "                                  role = role,\n",
    "                                  framework_version = '1.12',\n",
    "                                  entry_point = 'train.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6a) Host the SageMaker model and\n",
    "## 6b) Create an Endpoint to access the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the model. This can take ~10 minutes\n",
    "\n",
    "Ignore the message `update_endpoint is a no-op in sagemaker>=2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy a SageMaker to an endpoint\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our endpoint called?\n",
    "#endpoint = predictor.endpoint\n",
    "#endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success! You have deployed a keras model into AWS\n",
    "# ---------------------\n",
    "### 7. Confirm its working correctly by making a prediction\n",
    "Now, we want to use our endpoint/model. Create a predictor which uses the endpoint\n",
    "\n",
    "This step depends on what inputs your model is expecting. I simply used the iris dataset and so can feed it 4 inputs of which it will give me 3 probabilities - 1 for each iris type. \n",
    "\n",
    "Before deploying to aws, I got the predictions of my model - __locally__ - so that I could compare the local vs aws results (they should be the same). \n",
    "\n",
    "Locally, with the input below we get the following predictions:\n",
    "expected predictions:\n",
    "\n",
    "- 0.99930930\n",
    "- 0.00069067377\n",
    "- 0.00000000000000015728773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.999309, 0.000690674, 1.57288e-16]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a predictor which uses this new endpoint\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "endpoint = '' #get endpoint name from SageMaker > endpoints\n",
    "\n",
    "predictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint, sagemaker_session)\n",
    "# .predict send the data to our endpoint\n",
    "data = np.asarray([[5. , 3.5, 1.3, 0.3]]) #<-- update this to have inputs for your model\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup!\n",
    "\n",
    "else you will incur extra charges\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html\n",
    "\n",
    "- Stop Notebook\n",
    "- delete endpoints\n",
    "- delete models\n",
    "- delete S3 bucket\n",
    "- delete cloudwatch groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
